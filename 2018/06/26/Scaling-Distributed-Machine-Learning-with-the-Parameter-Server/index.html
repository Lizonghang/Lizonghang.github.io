<!doctype html>



  


<html class="theme-next pisces use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"/>




  <link href="//fonts.googleapis.com/css?family=Lato:300,400,700,400italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">



<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=0.5.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="https://assets-cdn.github.com/favicon.ico?v=0.5.0" />






<meta name="description" content="We described a parameter server framework to solve distributed machine learning problems. This framework is easy to use: Globally shared parameters can be used as local sparse vectors or matrices to p">
<meta property="og:type" content="article">
<meta property="og:title" content="Scaling Distributed Machine Learning with the Parameter Server">
<meta property="og:url" content="http://lizonghang.github.io/2018/06/26/Scaling-Distributed-Machine-Learning-with-the-Parameter-Server/index.html">
<meta property="og:site_name" content="努力学习天天向上">
<meta property="og:description" content="We described a parameter server framework to solve distributed machine learning problems. This framework is easy to use: Globally shared parameters can be used as local sparse vectors or matrices to p">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://lizonghang.github.io/img/1.png">
<meta property="og:image" content="http://lizonghang.github.io/img/2.png">
<meta property="og:image" content="http://lizonghang.github.io/img/3.png">
<meta property="og:image" content="http://lizonghang.github.io/img/4.png">
<meta property="og:image" content="http://lizonghang.github.io/img/5.png">
<meta property="og:image" content="http://lizonghang.github.io/img/6.png">
<meta property="og:image" content="http://lizonghang.github.io/img/7.png">
<meta property="og:image" content="http://lizonghang.github.io/img/8.png">
<meta property="og:image" content="http://lizonghang.github.io/img/9.png">
<meta property="og:image" content="http://lizonghang.github.io/img/10.png">
<meta property="og:image" content="http://lizonghang.github.io/img/11.png">
<meta property="og:image" content="http://lizonghang.github.io/img/12.png">
<meta property="og:image" content="http://lizonghang.github.io/img/13.png">
<meta property="og:updated_time" content="2018-06-26T10:46:34.207Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Scaling Distributed Machine Learning with the Parameter Server">
<meta name="twitter:description" content="We described a parameter server framework to solve distributed machine learning problems. This framework is easy to use: Globally shared parameters can be used as local sparse vectors or matrices to p">
<meta name="twitter:image" content="http://lizonghang.github.io/img/1.png">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Pisces',
    sidebar: {"position":"right","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>

  <title> Scaling Distributed Machine Learning with the Parameter Server | 努力学习天天向上 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-80234286-1', 'auto');
  ga('send', 'pageview');
</script>









  
  
    
  

  <div class="container one-collumn sidebar-position-right page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">努力学习天天向上</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">学习日志</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-home fa-fw"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-th fa-fw"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-archive fa-fw"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-user fa-fw"></i> <br />
            
            关于
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="#" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup">
 <span class="search-icon fa fa-search"></span>
 <input type="text" id="local-search-input">
 <div id="local-search-result"></div>
 <span class="popup-btn-close">close</span>
</div>


    </div>
  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Scaling Distributed Machine Learning with the Parameter Server
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-06-26T18:30:48+08:00" content="2018-06-26">
              2018-06-26
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/分布式AI/" itemprop="url" rel="index">
                    <span itemprop="name">分布式AI</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>We described a parameter server framework to solve distributed machine learning problems. This framework is easy to use: Globally shared parameters can be used as local sparse vectors or matrices to perform linear algebra operations with local training data. It is efficient: All com- munication is asynchronous. Flexible consistentcy models are supported to balance the trade-off between system efficiency and fast algorithm convergence rate. Furthermore, it provides elastic scalability and fault tolerance, aiming for stable long term deployment. Finally, we show experiments for several challenging tasks on real datasets with billions of variables to demonstrate its efficiency. We believe that this third generation parameter server is an important building block for scalable machine learning. The codes are available at parameterserver.org.</p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Realistic quantities of training data can range between 1TB and 1PB. This allows one to create powerful and complex models with 109 to 1012 parameters [9]. These models are often shared globally by all worker nodes, which must frequently accesses the shared parameters as they perform computation to refine it. Sharing imposes three challenges:</p>
<ul>
<li>Accessing the parameters requires an enormous amount of network bandwidth.</li>
<li>Many machine learning algorithms are sequential. The resulting barriers hurt performance when the cost of synchronization and machine latency is high.</li>
<li>At scale, fault tolerance is critical. Learning tasks are often performed in a cloud environment where machines can be unreliable and jobs can be preempted.</li>
</ul>
<h2 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h2><p>This paper describes a third generation open source implementation of a parameter server, which provides five key features:</p>
<ul>
<li><strong>Efficient communication:</strong> The aysnchronous communication model does not block computation (unless requested). It is optimized for machine learning tasks to reduce network traffic and overhead.</li>
<li><strong>Flexible consistency models:</strong> Relaxed consistency further hides synchronization cost and latency. We allow the algorithm designer to balance algorithmic convergence rate and system efficiency. The best trade-off depends on data, algorithm, and hardware.</li>
<li><strong>Elastic Scalability:</strong> Recovery from and repair of non-catastrophic machine failures within 1s, without interrupting computation. </li>
<li><strong>Ease of Use:</strong> The globally shared parameters are represented as (potentially sparse) vectors and matrices to facilitate development of machine learning applications.</li>
</ul>
<h2 id="Engineering-Challenges"><a href="#Engineering-Challenges" class="headerlink" title="Engineering Challenges"></a>Engineering Challenges</h2><p>When solving distributed data analysis problems, the issue of reading and updating parameters shared between different worker nodes is ubiquitous.The parameter server framework provides an efficient mechanism for aggregating and synchronizing model parameters and statistics between workers. Each parameter server node maintains only a part of the parameters, and each worker node typically requires only a subset of these parameters when operating. Here are two key challenges:</p>
<ul>
<li><strong>Communication:</strong> While the parameters could be updated as key-value pairs in a conventional datastore, using this abstraction naively is inefficient: values are typically small (floats or integers), and the overhead of sending each update as a key value operation is high.</li>
<li><strong>Fault tolerance:</strong> It must not require a full restart of a long-running computation. Live replication of parameters between servers supports hot failover. Failover and self-repair in turn support dynamic scaling by treating machine removal or addition as failure or repair respectively.</li>
</ul>
<h1 id="Machine-Learning"><a href="#Machine-Learning" class="headerlink" title="Machine Learning"></a>Machine Learning</h1><h2 id="distributed-subgradient-descent"><a href="#distributed-subgradient-descent" class="headerlink" title="distributed subgradient descent"></a>distributed subgradient descent</h2><center><img src="/img/1.png" style="width: 400px"></center>
<center><img src="/img/2.png" style="width: 400px"></center>

<p>the training data is partitioned among all of the workers, which jointly learn the parameter vector w. The algorithm operates iteratively. In each iteration, every worker independently uses its own training data to determine what changes should be made to w in order to get closer to an optimal value. Because each worker’s updates reflect only its own training data, the system needs a mechanism to allow these updates to mix. It does so by expressing the updates as a subgradient—a direction in which the parameter vector w should be shifted—and aggregates all subgradients before applying them to w. These gradients are typically scaled down, with considerable attention paid in algorithm design to the right learning rate η that should be applied in order to ensure that the algorithm converges quickly.</p>
<p>The most expensive step is computing the subgradient to update w. This task is divided among all of the workers, each of which execute WORKERITERATE. As part of this, workers compute w<sup>T</sup>x<sub>ik</sub> , which could be infeasible for very high-dimensional w. Fortunately, a worker needs to know a coordinate of w if and only if some of its training data references that entry.</p>
<center><img src="/img/3.png" style="width: 400px"></center>

<p>For 100 workers, each worker only needs 7.8% of the total parameters. With 10,000 workers this reduces to 0.15%.</p>
<h1 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h1><p>An instance of the parameter server can run more than one algorithm simultaneously. Parameter server nodes are grouped into a server group and several worker groups as shown below.</p>
<center><img src="/img/4.png" style="width: 400px"></center>

<p>A server node in the server group maintains a partition of the globally shared parameters. Server nodes communicate with each other to replicate and/or to migrate parameters for reliability and scaling. A server manager node maintains a consistent view of the metadata of the servers, such as node liveness and the assignment of parameter partitions.</p>
<p>Each worker group runs an application. A worker typically stores locally a portion of the training data to compute local statistics such as gradients. Workers communicate only with the server nodes (not among themselves), updating and retrieving the shared parameters. There is a scheduler node for each worker group. It assigns tasks to workers and monitors their progress. If workers are added or removed, it reschedules unfinished tasks.</p>
<p>The parameter server supports independent parameter namespaces. This allows a worker group to isolate its set of shared parameters from others. Several worker groups may also share the same namespace: we may use more than one worker group to solve the same deep learning application to increase parallelization.</p>
<h2 id="Key-Value-Vectors"><a href="#Key-Value-Vectors" class="headerlink" title="(Key,Value) Vectors"></a>(Key,Value) Vectors</h2><p>The model shared among nodes can be represented as a set of (key, value) pairs. Each entry of the model can be read and written locally or remotely by its key and we assume that the keys are ordered.</p>
<h2 id="Range-Push-and-Pull"><a href="#Range-Push-and-Pull" class="headerlink" title="Range Push and Pull"></a>Range Push and Pull</h2><p>The parameter server optimizes these updates for programmer convenience as well as computational and network bandwidth efficiency by supporting range-based push and pull. If R is a key range, then w.push(R, dest) sends all existing entries of w in key range R to the destination, which can be either a particular node, or a node group such as the server group. Similarly, w.pull(R, dest) reads all existing entries of w in key range R from the destination. If we set R to be the whole key range, then the whole vector w will be communicated. If we set R to include a single key, then only an individual entry will be sent.</p>
<h2 id="User-Defined-Functions-on-the-Server"><a href="#User-Defined-Functions-on-the-Server" class="headerlink" title="User-Defined Functions on the Server"></a>User-Defined Functions on the Server</h2><p>Beyond aggregating data from workers, server nodes can execute user-defined functions.</p>
<h2 id="Asynchronous-Tasks-and-Dependency"><a href="#Asynchronous-Tasks-and-Dependency" class="headerlink" title="Asynchronous Tasks and Dependency"></a>Asynchronous Tasks and Dependency</h2><p> Tasks may include any number of subtasks. For example, the task WorkerIterate in Algorithm 1 contains one push and one pull. Tasks are executed asynchronously: the caller can perform further computation immediately after issuing a task.</p>
<p>The caller marks a task as finished only once it receives the callee’s reply. A reply could be the function return of a user-defined function, the (key, value) pairs requested by the pull, or an empty acknowledgement. The callee marks a task as finished only if the call of the task is returned and all subtasks issued by this call are finished.</p>
<center><img src="/img/5.png" style="width: 500px"></center>

<p>By default, callees execute tasks in parallel, for best performance. Figure 5 depicts three example iterations of WorkerIterate. Iterations 10 and 11 are independent, but 12 depends on 11. The callee therefore begins iteration 11 immediately after the local gradients are computed in iteration 10. Iteration 12, however, is postponed until the pull of 11 finishes.</p>
<h2 id="Flexible-Consistency"><a href="#Flexible-Consistency" class="headerlink" title="Flexible Consistency"></a>Flexible Consistency</h2><p>Independent tasks improve system efficiency via parallelizing the use of CPU, disk and network bandwidth. However, this may lead to data inconsistency between nodes. The best trade-off between system efficiency and algorithm convergence rate usually depends on a variety of factors, including the algorithm’s sensitivity to data inconsistency, feature correlation in training data, and capacity difference of hardware components. Instead of forcing the user to adopt one particular dependency that may be illsuited to the problem, the parameter server gives the algorithm designer flexibility in defining consistency models.</p>
<center><img src="/img/6.png" style="width: 500px"></center>

<ul>
<li><strong>Sequential:</strong> Also named BSP. In sequential consistency, all tasks are executed one by one. The next task can be started only if the previous one has finished. It produces results identical to the single-thread implementation.</li>
<li><strong>Eventual:</strong> Similar to TSP. All tasks may be started simultaneously. However, this is only recommendable if the underlying algorithms are robust with regard to delays.</li>
<li><strong>Bounded Delay:</strong> Similar to SSP. When a maximal delay time τ is set, a new task will be blocked until all previous tasks τ times ago have been finished. τ = 0 is the sequential consistency model, and an infinite delay τ = ∞ becomes the eventual consistency model.</li>
</ul>
<p>Note that the dependency graphs may be dynamic. For instance the scheduler may increase or decrease the maximal delay according to the runtime progress to balance system efficiency and convergence of the underlying optimization algorithm. In this case the caller traverses the DAG.</p>
<h2 id="User-defined-Filters"><a href="#User-defined-Filters" class="headerlink" title="User-defined Filters"></a>User-defined Filters</h2><p>The parameter server supports user-defined filters to selectively synchronize individual (key,value) pairs, allowing fine-grained control of data consistency within a task. </p>
<p>One example is the significantly modified filter, which only pushes entries that have changed by more than a threshold since their last synchronization. Later, we discuss another filter named KKT which takes advantage of the optimality condition of the optimization problem: a worker only pushes gradients that are likely to affect the weights on the servers.</p>
<h1 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h1><h2 id="Vector-Clock"><a href="#Vector-Clock" class="headerlink" title="Vector Clock"></a>Vector Clock</h2><p>vector clock records the time of each individual node on this (key,value) pair. Vector clocks are convenient, e.g., for tracking aggregation status or rejecting doubly sent data. However, a naive implementation of the vector clock requires O(nm) space to handle n nodes and m parameters. With thousands of nodes and billions of parameters, this is infeasible in terms of memory and bandwidth.</p>
<p>Many parameters share the same timestamp as a result of the range-based communication pattern of the parameter server: If a node pushes the parameters in a range, then the timestamps of the parameters associated with the node are likely the same. Therefore, they can be compressed into a single range vector clock.</p>
<p>Initially, there is only one range vector clock for each node i. It covers the entire parameter key space as its range with 0 as its initial timestamp. Each range set may split the range and create at most 3 new vector clocks. Let k be the total number of unique ranges communicated by the algorithm, then there are at most O(mk) vector clocks, where m is the number of nodes. k is typically much smaller than the total number of parameters. This significantly reduces the space required for range vector clocks.</p>
<h2 id="Messages"><a href="#Messages" class="headerlink" title="Messages"></a>Messages</h2><p>Nodes may send messages to individual nodes or node groups. A message consists of a list of (key,value) pairs in the key range R and the associated range vector clock:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[vc(R), (k1,v1), ..., (kp,vp)] kj ∈ R and j ∈ &#123;1,...p&#125;</div></pre></td></tr></table></figure>
<p>For tasks, a (key,value) pair might assume the form (task ID, arguments or return results).</p>
<p>Because machine learning problems typically require high bandwidth, message compression is desirable. Training data often remains unchanged between iterations. A worker might send the same key lists again. Hence it is desirable for the receiving node to cache the key lists. Later, the sender only needs to send a hash of the list rather than the list itself.</p>
<p>Values may contain many zero entries. For example, a large portion of parameters remain unchanged in sparse logistic regression. Likewise, a user-defined filter may also zero out a large fraction of the values. Hence we need only send non-zero (key,value) pairs. We use the fast Snappy compression library to compress messages, effectively removing the zeros.</p>
<p>Key-caching and value-compression can be used jointly.</p>
<h2 id="Consistent-Hashing"><a href="#Consistent-Hashing" class="headerlink" title="Consistent Hashing"></a>Consistent Hashing</h2><center><img src="/img/7.png" style="width: 200px"></center>

<p>In parameter server, keys and server node IDs are both inserted into the hash ring. Each server node manages the key range starting with its insertion point to the next point by other nodes in the counter-clockwise direction. This node is called the master of this key range. A physical server is often represented in the ring via multiple “virtual” servers to improve load balancing and recovery.</p>
<h2 id="Replication-and-Consistency"><a href="#Replication-and-Consistency" class="headerlink" title="Replication and Consistency"></a>Replication and Consistency</h2><p>Each server node stores a replica of the k counterclock-wise neighbor key ranges relative to the one it owns. We refer to nodes holding copies as slaves of the appropriate key range. The above diagram shows an example with k = 2, where server 1 replicates the key ranges owned by server 2 and server 3.</p>
<p>Worker nodes communicate with the master of a key range for both push and pull. Any modification on the master is copied with its timestamp to the slaves. Modifications to data are pushed synchronously to the slaves.</p>
<center><img src="/img/8.png" style="width: 600px"></center>

<p>Figure 8 shows a case where worker 1 pushes x into server 1, which invokes a user defined function f to modify the shared data. The push task is completed only once the data modification f (x) is copied to the slave.</p>
<p>Naive replication potentially increases the network traffic by k times. This is undesirable for many machine learning applications that depend on high network bandwidth. The parameter server framework permits replication after aggregation. Server nodes often aggregate data from the worker nodes, such as summing local gradients. Servers may therefore postpone replication until aggregation is complete.</p>
<p>In the righthand side of the diagram, two workers push x and y to the server, respectively. The server first aggregates the push by x + y, then applies the modification f (x + y), and finally performs the replication. With n workers, replication uses only k/n bandwidth. Often k is a small constant, while n is hundreds to thousands. While aggregation increases the delay of the task reply, it can be hidden by relaxed consistency conditions.</p>
<h2 id="Server-Management"><a href="#Server-Management" class="headerlink" title="Server Management"></a>Server Management</h2><p>To achieve fault tolerance and dynamic scaling we must support addition and removal of nodes. The following steps happen when a server joins:</p>
<ol>
<li>The server manager assigns the new node a key range to serve as master. This may cause another key range to split or be removed from a terminated node.</li>
<li>The node fetches the range of data to maintains as master and k additional ranges to keep as slave.</li>
<li>The server manager broadcasts the node changes. The recipients of the message may shrink their own data based on key ranges they no longer hold and to resubmit unfinished tasks to the new node.</li>
</ol>
<p>Fetching the data in the range R from some node S proceeds in two stages:</p>
<ol>
<li>First, S pre-copies all (key,value) pairs in the range together with the associated vector clocks. This may cause a range vector clock to split. If the new node fails at this stage, S remains unchanged. </li>
<li>Second, S no longer accepts messages affecting the key range R by dropping the messages without executing and replying. At the same time, S sends the new node all changes that occurred in R during the pre-copy stage.</li>
</ol>
<p>On receiving the node change message a node N first checks if it also maintains the key range R. If true and if this key range is no longer to be maintained by N, it deletes all associated (key,value) pairs and vector clocks in R. Next, N scans all outgoing messages that have not received replies yet. If a key range intersects with R, then the message will be split and resent.</p>
<p>Due to delays, failures, and lost acknowledgements N may send messages twice. Due to the use of vector clocks both the original recipient and the new node are able to reject this message and it does not affect correctness.</p>
<p>The server manager tasks a new node with taking the key range of the leaving node. The server manager detects node failure by a heartbeat sig- nal. Integration with a cluster resource manager such as Yarn or Mesos is left for future work.</p>
<h2 id="Worker-Management"><a href="#Worker-Management" class="headerlink" title="Worker Management"></a>Worker Management</h2><p>Adding a new worker node W is similar but simpler than adding a new server node:</p>
<ol>
<li>The task scheduler assigns W a range of data.</li>
<li>This node loads the range of training data from a network file system or existing workers. Next, W pulls the shared parameters from servers.</li>
<li>The task scheduler broadcasts the change, possibly causing other workers to free some training data.</li>
</ol>
<p>When a worker departs, the task scheduler may start a replacement. We give the algorithm designer the option to control recovery for two reasons:</p>
<ul>
<li>If the training data is huge, recovering a worker node be may more expensive than recovering a server node.</li>
<li>Losing a small amount of training data during optimization typically affects the model only a little. Hence the algorithm designer may prefer to continue without replacing a failed worker. It may even be desirable to terminate the slowest workers.</li>
</ul>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><p>Sparse logistic regression is one of the most popular algorithms for large scale risk minimization. It combines the logistic loss with the l1 regularizer, which biases a compact solution with a large portion of 0 value entries. </p>
<p>We collected an ad click prediction dataset with 170 billion examples and 65 billion unique features. This dataset is 636 TB uncompressed (141 TB compressed). We ran the parameter server on 1000 machines, each with 16 physical cores, 192GB DRAM, and connected by 10 Gb Ethernet. 800 machines acted as workers, and 200 were parameter servers. The cluster was in concurrent use by other (unrelated) tasks during operation.</p>
<center><img src="/img/9.png" style="width: 400px"></center>

<p>We used a state-of-the-art distributed regression algorithm (Algorithm 3), In which:</p>
<ul>
<li>First, only a block of parameters is updated in an iteration.</li>
<li>Second, the workers compute both gradients and the diagonal part of the second derivative on this block.</li>
<li>Third, the parameter servers themselves must perform complex computation: the servers update the model by solving a proximal operator based on the aggregated local gradients.</li>
<li>Fourth, we use a bounded-delay model over iterations and use a “KKT” filter to suppress transmission of parts of the generated gradient update that are small enough that their effect is likely to be negligible.</li>
</ul>
<p>We compare the parameter server with two special-purpose systems, named System A and B, developed by a large internet company. Both Systems A and B consist of more than10K lines of code. The parameter server only requires 300 lines of code for the same functionality as System B.</p>
<center><img src="/img/10.png" style="width: 300px"></center>

<p>We first compare these three systems by running them to reach the same objective value. System B outperforms system A because it uses a better algorithm. The parameter server outperforms System B while using the same algorithm, because of the efficacy of reducing the network traffic and the relaxed consistency model.</p>
<center><img src="/img/11.png" style="width: 300px"></center>

<p>Figure 10 shows that the relaxed consistency model substantially increases worker node utilization. Workers in System A are 32% idle, and in system B, they are 53% idle, while waiting for the barrier in each block. The parameter server reduces this cost to under 2%.</p>
<center><img src="/img/12.png" style="width: 600px"></center>

<p>Figure 11 shows the reduction of network traffic by each system components for servers and workers. As can be seen, allowing the senders and receivers to cache the keys can save near 50% traffic. This is because both key (int64) and value (double) are of the same size, and the key set is not changed during optimization. In addition, data compression is effective for compressing the values for both servers (&gt;20x) and workers when applying the KKT filter (&gt;6x). The reason is twofold. First, the l1 regularizer encourages a sparse model (w), so that most of values pulled from servers are 0. Second, the KKT filter forces a large portion of gradients sending to servers to be 0.</p>
<center><img src="/img/13.png" style="width: 300px"></center>

<p>Finally, we analyze the bounded delay consistency model. The time decomposition of workers to achieve the same convergence criteria under different maximum allowed delay (τ ) is shown in Figure 13. As expected, the waiting time decreases when the allowed delay increases. Workers are 50% idle when using the sequential consistency model (τ = 0), while the idle rate is reduced to 1.7% when τ is set to be 16. However, the computing time increases nearly linearly with τ. Because the data inconsistency slows convergence, more iterations are needed to achieve the same convergence criteria. As a result, τ = 8 is the best trade-off between algorithm convergence and system performance.</p>

      
    </div>

    <div>
      
        
      
    </div>

    <footer class="post-footer">
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/05/23/Inception-ResNet-v2/" rel="next" title="Inception-ResNet-v2">
                <i class="fa fa-chevron-left"></i> Inception-ResNet-v2
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.jpg"
               alt="李宗航" />
          <p class="site-author-name" itemprop="name">李宗航</p>
          <p class="site-description motion-element" itemprop="description">学习日志</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives/">
              <span class="site-state-item-count">58</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories/">
                <span class="site-state-item-count">10</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/lizonghang/" target="_blank">
                  
                    <i class="fa fa-globe"></i>
                  
                  github
                </a>
              </span>
            
          
        </div>

        
        

        
        <div class="links-of-blogroll motion-element">
          
        </div>

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Contributions"><span class="nav-number">1.1.</span> <span class="nav-text">Contributions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Engineering-Challenges"><span class="nav-number">1.2.</span> <span class="nav-text">Engineering Challenges</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Machine-Learning"><span class="nav-number">2.</span> <span class="nav-text">Machine Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#distributed-subgradient-descent"><span class="nav-number">2.1.</span> <span class="nav-text">distributed subgradient descent</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Architecture"><span class="nav-number">3.</span> <span class="nav-text">Architecture</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Key-Value-Vectors"><span class="nav-number">3.1.</span> <span class="nav-text">(Key,Value) Vectors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Range-Push-and-Pull"><span class="nav-number">3.2.</span> <span class="nav-text">Range Push and Pull</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#User-Defined-Functions-on-the-Server"><span class="nav-number">3.3.</span> <span class="nav-text">User-Defined Functions on the Server</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Asynchronous-Tasks-and-Dependency"><span class="nav-number">3.4.</span> <span class="nav-text">Asynchronous Tasks and Dependency</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Flexible-Consistency"><span class="nav-number">3.5.</span> <span class="nav-text">Flexible Consistency</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#User-defined-Filters"><span class="nav-number">3.6.</span> <span class="nav-text">User-defined Filters</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Implementation"><span class="nav-number">4.</span> <span class="nav-text">Implementation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Vector-Clock"><span class="nav-number">4.1.</span> <span class="nav-text">Vector Clock</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Messages"><span class="nav-number">4.2.</span> <span class="nav-text">Messages</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Consistent-Hashing"><span class="nav-number">4.3.</span> <span class="nav-text">Consistent Hashing</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Replication-and-Consistency"><span class="nav-number">4.4.</span> <span class="nav-text">Replication and Consistency</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Server-Management"><span class="nav-number">4.5.</span> <span class="nav-text">Server Management</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Worker-Management"><span class="nav-number">4.6.</span> <span class="nav-text">Worker Management</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Evaluation"><span class="nav-number">5.</span> <span class="nav-text">Evaluation</span></a></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">李宗航</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>



      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  


  




<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  <script type="text/javascript" src="/vendors/velocity/velocity.min.js"></script>

  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js"></script>

  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=0.5.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=0.5.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=0.5.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=0.5.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=0.5.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=0.5.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=0.5.0"></script>



  



  



  
  
  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
       search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body").append('<div class="popoverlay">').css('overflow', 'hidden');
      $('.popup').toggle();

    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
    'use strict';
    $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
            // get the contents from search data
            isfetched = true;
            $('.popup').detach().appendTo('.header-inner');
            var datas = $( "entry", xmlResponse ).map(function() {
                return {
                    title: $( "title", this ).text(),
                    content: $("content",this).text(),
                    url: $( "url" , this).text()
                };
            }).get();
            var $input = document.getElementById(search_id);
            var $resultContent = document.getElementById(content_id);
            $input.addEventListener('input', function(){
                var matchcounts = 0;
                var str='<ul class=\"search-result-list\">';                
                var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                $resultContent.innerHTML = "";
                if (this.value.trim().length > 1) {
                // perform local searching
                datas.forEach(function(data) {
                    var isMatch = true;
                    var content_index = [];
                    var data_title = data.title.trim().toLowerCase();
                    var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                    var data_url = data.url;
                    var index_title = -1;
                    var index_content = -1;
                    var first_occur = -1;
                    // only match artiles with not empty titles and contents
                    if(data_title != '' && data_content != '') {
                        keywords.forEach(function(keyword, i) {
                            index_title = data_title.indexOf(keyword);
                            index_content = data_content.indexOf(keyword);
                            if( index_title < 0 && index_content < 0 ){
                                isMatch = false;
                            } else {
                                if (index_content < 0) {
                                    index_content = 0;
                                }
                                if (i == 0) {
                                    first_occur = index_content;
                                }
                            }
                        });
                    }
                    // show search results
                    if (isMatch) {
                        matchcounts += 1;
                        str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                        var content = data.content.trim().replace(/<[^>]+>/g,"");
                        if (first_occur >= 0) {
                            // cut out 100 characters
                            var start = first_occur - 20;
                            var end = first_occur + 80;
                            if(start < 0){
                                start = 0;
                            }
                            if(start == 0){
                                end = 50;
                            }
                            if(end > content.length){
                                end = content.length;
                            }
                            var match_content = content.substring(start, end);
                            // highlight all keywords
                            keywords.forEach(function(keyword){
                                var regS = new RegExp(keyword, "gi");
                                match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                            });
                            
                            str += "<p class=\"search-result\">" + match_content +"...</p>"
                        }
                        str += "</li>";
                    }
                })};
                str += "</ul>";
                if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
                if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
                $resultContent.innerHTML = str;
            });
            proceedsearch();
        }
    });}

    // handle and trigger popup window;
    $('.popup-trigger').mousedown(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };

    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".popoverlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>

  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>

  
    <script type="text/javascript" src="http://cdn.staticfile.org/mathjax/2.4.0/MathJax.js"></script>
    <script type="text/javascript" src="http://cdn.staticfile.org/mathjax/2.4.0/config/TeX-AMS-MML_HTMLorMML.js"></script>
  


  

  

</body>
</html>
